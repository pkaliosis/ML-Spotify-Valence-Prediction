{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c2a6bf",
   "metadata": {},
   "source": [
    "<font size=3> Applied Machine Learning - Dissecting the Spotify valence.\n",
    "    <br> Panagiotis Kaliosis - p3180061 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f48dc",
   "metadata": {},
   "source": [
    "<font size =3><br> We begin by importing the libraries that we are going to need. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eec4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotnine import *\n",
    "import seaborn as sns\n",
    "import scipy.stats.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "510ddd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ggplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6930d",
   "metadata": {},
   "source": [
    "<font size=3><br> Loading our dataset...</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abf411a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>song_id</th>\n",
       "      <th>song_name</th>\n",
       "      <th>artist</th>\n",
       "      <th>streams</th>\n",
       "      <th>last_week_position</th>\n",
       "      <th>weeks_on_chart</th>\n",
       "      <th>peak_position</th>\n",
       "      <th>position_status</th>\n",
       "      <th>week_start</th>\n",
       "      <th>week_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5aAx2yezTd8zXrkmtKl66Z</td>\n",
       "      <td>Starboy</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>25734078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>new</td>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>2017-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7BKLCZ1jbUBVqRi2FVlTVw</td>\n",
       "      <td>Closer</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>23519705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>new</td>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>2017-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>Rockabye (feat. Sean Paul &amp; Anne-Marie)</td>\n",
       "      <td>Clean Bandit</td>\n",
       "      <td>21216399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>new</td>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>2017-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4pdPtRcBmOSQDlJ3Fk945m</td>\n",
       "      <td>Let Me Love You</td>\n",
       "      <td>DJ Snake</td>\n",
       "      <td>19852704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>new</td>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>2017-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3NdDpSvN911VPGivFlV5d0</td>\n",
       "      <td>I Don’t Wanna Live Forever (Fifty Shades Darke...</td>\n",
       "      <td>ZAYN</td>\n",
       "      <td>18316326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>new</td>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>2017-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196</td>\n",
       "      <td>0fySG6A6qLE8IvDpayb5bM</td>\n",
       "      <td>VIBEZ</td>\n",
       "      <td>DaBaby</td>\n",
       "      <td>5560742</td>\n",
       "      <td>158.0</td>\n",
       "      <td>13</td>\n",
       "      <td>53</td>\n",
       "      <td>-38</td>\n",
       "      <td>2019-12-20</td>\n",
       "      <td>2019-12-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197</td>\n",
       "      <td>7CZyCXKG6d5ALeq41sLzbw</td>\n",
       "      <td>Take What You Want (feat. Ozzy Osbourne &amp; Trav...</td>\n",
       "      <td>Post Malone</td>\n",
       "      <td>5548309</td>\n",
       "      <td>99.0</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>-98</td>\n",
       "      <td>2019-12-20</td>\n",
       "      <td>2019-12-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>198</td>\n",
       "      <td>5a6pdCHlWS2ekOOQ70QnAr</td>\n",
       "      <td>July</td>\n",
       "      <td>Noah Cyrus</td>\n",
       "      <td>5525805</td>\n",
       "      <td>156.0</td>\n",
       "      <td>7</td>\n",
       "      <td>138</td>\n",
       "      <td>-42</td>\n",
       "      <td>2019-12-20</td>\n",
       "      <td>2019-12-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>199</td>\n",
       "      <td>25ZAibhr3bdlMCLmubZDVt</td>\n",
       "      <td>QUE PRETENDES</td>\n",
       "      <td>J Balvin</td>\n",
       "      <td>5502509</td>\n",
       "      <td>177.0</td>\n",
       "      <td>26</td>\n",
       "      <td>15</td>\n",
       "      <td>-22</td>\n",
       "      <td>2019-12-20</td>\n",
       "      <td>2019-12-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>200</td>\n",
       "      <td>3TKpJrY9q49Mj1JOsM9zGL</td>\n",
       "      <td>Family</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>5453653</td>\n",
       "      <td>144.0</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>-56</td>\n",
       "      <td>2019-12-20</td>\n",
       "      <td>2019-12-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30800 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     position                 song_id  \\\n",
       "0           1  5aAx2yezTd8zXrkmtKl66Z   \n",
       "1           2  7BKLCZ1jbUBVqRi2FVlTVw   \n",
       "2           3  5knuzwU65gJK7IF5yJsuaW   \n",
       "3           4  4pdPtRcBmOSQDlJ3Fk945m   \n",
       "4           5  3NdDpSvN911VPGivFlV5d0   \n",
       "..        ...                     ...   \n",
       "195       196  0fySG6A6qLE8IvDpayb5bM   \n",
       "196       197  7CZyCXKG6d5ALeq41sLzbw   \n",
       "197       198  5a6pdCHlWS2ekOOQ70QnAr   \n",
       "198       199  25ZAibhr3bdlMCLmubZDVt   \n",
       "199       200  3TKpJrY9q49Mj1JOsM9zGL   \n",
       "\n",
       "                                             song_name            artist  \\\n",
       "0                                              Starboy        The Weeknd   \n",
       "1                                               Closer  The Chainsmokers   \n",
       "2              Rockabye (feat. Sean Paul & Anne-Marie)      Clean Bandit   \n",
       "3                                      Let Me Love You          DJ Snake   \n",
       "4    I Don’t Wanna Live Forever (Fifty Shades Darke...              ZAYN   \n",
       "..                                                 ...               ...   \n",
       "195                                              VIBEZ            DaBaby   \n",
       "196  Take What You Want (feat. Ozzy Osbourne & Trav...       Post Malone   \n",
       "197                                               July        Noah Cyrus   \n",
       "198                                      QUE PRETENDES          J Balvin   \n",
       "199                                             Family  The Chainsmokers   \n",
       "\n",
       "      streams  last_week_position  weeks_on_chart  peak_position  \\\n",
       "0    25734078                 NaN               1              1   \n",
       "1    23519705                 NaN               1              2   \n",
       "2    21216399                 NaN               1              3   \n",
       "3    19852704                 NaN               1              4   \n",
       "4    18316326                 NaN               1              5   \n",
       "..        ...                 ...             ...            ...   \n",
       "195   5560742               158.0              13             53   \n",
       "196   5548309                99.0              15             16   \n",
       "197   5525805               156.0               7            138   \n",
       "198   5502509               177.0              26             15   \n",
       "199   5453653               144.0               3            105   \n",
       "\n",
       "    position_status week_start   week_end  \n",
       "0               new 2016-12-30 2017-01-06  \n",
       "1               new 2016-12-30 2017-01-06  \n",
       "2               new 2016-12-30 2017-01-06  \n",
       "3               new 2016-12-30 2017-01-06  \n",
       "4               new 2016-12-30 2017-01-06  \n",
       "..              ...        ...        ...  \n",
       "195             -38 2019-12-20 2019-12-27  \n",
       "196             -98 2019-12-20 2019-12-27  \n",
       "197             -42 2019-12-20 2019-12-27  \n",
       "198             -22 2019-12-20 2019-12-27  \n",
       "199             -56 2019-12-20 2019-12-27  \n",
       "\n",
       "[30800 rows x 11 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = 0\n",
    "dfs = []\n",
    "for file in glob.glob('Charts/global/201?/*.csv'):\n",
    "    dates = re.findall('\\d{4}-\\d{2}-\\d{2}', file.split('/')[-1])\n",
    "    weekly_chart = pd.read_csv(file, header=header, sep='\\t')\n",
    "    weekly_chart['week_start'] = datetime.strptime(dates[0], '%Y-%m-%d')\n",
    "    weekly_chart['week_end'] = datetime.strptime(dates[1], '%Y-%m-%d')\n",
    "    dfs.append(weekly_chart)\n",
    "\n",
    "all_charts = pd.concat(dfs)\n",
    "all_charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abc32f",
   "metadata": {},
   "source": [
    "<font size=3><br> Checking the distinct number of songs that can be found in the dataset and dropping the duplicates. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7ee5adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3021"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_charts['song_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee545cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_charts = all_charts.drop_duplicates('song_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c0336e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3021"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_charts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a0e6c9",
   "metadata": {},
   "source": [
    "<font size=3><br> So the size of the dataset is 3021 unique songs. \n",
    "<br> Now it is time to load the tracks' features. We will use spotipy library, creating a Spotify client in order to access the data. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd68cc6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spotify_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16864/2883880669.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mspotify_config\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m client_credentials_manager = SpotifyClientCredentials(config['client_id'],\n\u001b[0;32m      4\u001b[0m                                                       config['client_secret'])\n\u001b[0;32m      5\u001b[0m \u001b[0msp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspotipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSpotify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient_credentials_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclient_credentials_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spotify_config'"
     ]
    }
   ],
   "source": [
    "from spotify_config import config\n",
    "\n",
    "client_credentials_manager = SpotifyClientCredentials(config['client_id'],\n",
    "                                                      config['client_secret'])\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "#sp.trace = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d3dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "all_track_ids = list(all_charts['song_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb17711",
   "metadata": {},
   "source": [
    "<font size=3><br> Loading the tracks' features in 100-sized batches. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "num_tracks = 100\n",
    "while start < len(all_track_ids):\n",
    "    print(f'getting from {start} to {start+num_tracks}')\n",
    "    tracks_batch = all_track_ids[start:start+num_tracks]\n",
    "    features_batch = sp.audio_features(tracks_batch)\n",
    "    features.update({ track_id : track_features \n",
    "                     for track_id, track_features in zip(tracks_batch, features_batch) })\n",
    "    start += num_tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb842fe",
   "metadata": {},
   "source": [
    "<font size=3><br> Let's see an example of a song's features. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['7qiZfU4dY1lWllzX7mPBI3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd8b46",
   "metadata": {},
   "source": [
    "<font size=3><br> Turning the dictionary into a DataFrame.\n",
    "<br> Source: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.from_dict.html </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044b6971",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = pd.DataFrame.from_dict(features, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b3567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb9d55d",
   "metadata": {},
   "source": [
    "<font size=3><br> Dropping the unnecessary columns, as they are unable to help us for the goals of this assignment. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093adb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats=feats.drop(columns=['type', 'id', 'uri', 'track_href', 'analysis_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5948f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = feats.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f976b5",
   "metadata": {},
   "source": [
    "<font size=3><br> Planning on joining the \"song_id\" and \"song_name\" columns of the all_charts dataframe into the feats dataframe. So I extract these two columns into a separate dataframe. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4246ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_names = all_charts[['song_id', 'song_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa7d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa2844",
   "metadata": {},
   "source": [
    "<font size=3><br> Joining the two dataframes on the \"song_id\" attribute.\n",
    "<br> Source: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3acd033",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = feats.join(song_names.set_index('song_id'), on='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe59dd6",
   "metadata": {},
   "source": [
    "<font size=3><br>Now moving the \"song_name\" column at the beginning of the dataframe in order to look better. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25370c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_first = feats.pop('song_name')\n",
    "feats.insert(0, 'song_name', move_first)\n",
    "feats.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222cc282",
   "metadata": {},
   "source": [
    "<font size=4><br> Q1 - Explore which Features Influence Valence </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.columns[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520c585e",
   "metadata": {},
   "source": [
    "<font size=3><br> A common way to measure correlation between two or more variables is the Pearson correlation coefficient.\n",
    "So, the script below is used to measure the Pearson correlation coefficient between the valence and all of the other track features. You can read more about Pearson's correlation here: https://stackabuse.com/calculating-pearson-correlation-coefficient-in-python-with-numpy/ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d294d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson = pd.DataFrame(columns=['feature', 'correlation'])\n",
    "X = feats.valence\n",
    "for item in range(2, 12):\n",
    "    if item != 11:\n",
    "        Y = feats[feats.columns[item]]\n",
    "        out = stats.pearsonr(X,Y)\n",
    "        row = {'feature': feats.columns[item], 'correlation': out[0]}\n",
    "        pearson = pearson.append(row, ignore_index = True)\n",
    "\n",
    "pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee114fe",
   "metadata": {},
   "source": [
    "<font size=3><br>As we can see, there is no strong linear correlation with any of the other features. The stronger ones are with energy, danceability and loudness, although they are not that strong. But this was a really simple approach. We are going to be more advanced.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99492d18",
   "metadata": {},
   "source": [
    "<font size=3><br> The stronger linear correlation is the one between valence and energy. But it still does not look really linear. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6db7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot() + geom_point(aes(x=feats.valence, y=feats.energy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae3f31",
   "metadata": {},
   "source": [
    "<font size=3><br> Let's try Spearman's correlation, which is another metric for measuring the correlation between 2 or more variables. The Spearman's correlation is more sensitive to outliers.\n",
    "<br> More: https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18598c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman = pd.DataFrame(columns=['feature', 'correlation'])\n",
    "X = feats.valence\n",
    "for item in range(2, 12):\n",
    "    if item != 11:\n",
    "        Y = feats[feats.columns[item]]\n",
    "        out = stats.spearmanr(X,Y)\n",
    "        row = {'feature': feats.columns[item], 'correlation': out[0]}\n",
    "        spearman = spearman.append(row, ignore_index = True)\n",
    "\n",
    "spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05665f49",
   "metadata": {},
   "source": [
    "<font size=3><br> The Spearman's correlation results are more or less the same. As a first approach, we could say that danceability, energy and loudness are the three features that look more correlated with the valence. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea6076",
   "metadata": {},
   "source": [
    "<font size=3><br> Regression </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4161c146",
   "metadata": {},
   "source": [
    "<font size=3><br> We'll start by performing a regression of the valence value on the energy variable. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd51820",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(\"valence ~ energy\", data=feats)\n",
    "feats_res = mod.fit()\n",
    "feats_res.params\n",
    "feats_res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087cb177",
   "metadata": {},
   "source": [
    "<font size=3><br> The R-squared metric is 0.135, which is not really high. Let's plot the data and the regression line. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6bee99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ggplot(data=feats) +\\\n",
    "    geom_point(mapping=aes(x='energy', y='valence')) +\\\n",
    "    geom_abline(intercept=feats_res.params['Intercept'], \n",
    "                slope=feats_res.params['energy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27558c",
   "metadata": {},
   "source": [
    "<font size=3><br> Now let's plot the residuals in order to get a better undestanding of the situation. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data=feats) +\\\n",
    "    geom_hline(yintercept=0, color='black', linetype='dotted') +\\\n",
    "    geom_point(mapping=aes(x='energy', y=feats_res.resid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d9cc1",
   "metadata": {},
   "source": [
    "<font size=3><br> And of course the QQ-plot. There seems to be a correlation between the two variables but it is not that strong. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data=feats_res.resid.to_frame().rename(columns={0: 'resid'})) +\\\n",
    "    geom_qq(mapping=aes(sample='resid')) +\\\n",
    "    geom_qq_line(mapping=aes(sample='resid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ecffcd",
   "metadata": {},
   "source": [
    "<font size=3><br> We will now proceed to add the danceability feature on our regression. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f277e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_ml = smf.ols(\"valence ~ energy + danceability\", data=feats)\n",
    "feats_res_ml = mod_ml.fit()\n",
    "feats_res_ml.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451c7000",
   "metadata": {},
   "source": [
    "<font size=3><br> The R-squared metric is a bit better now, with a value of 0.205. But still not really satisfactory. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751492b6",
   "metadata": {},
   "source": [
    "<font size=3><br> And finally, the loudness feature is added into the mix. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf29c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_ml = smf.ols(\"valence ~ energy + danceability + loudness\", data=feats)\n",
    "feats_res_ml = mod_ml.fit()\n",
    "feats_res_ml.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31925d9b",
   "metadata": {},
   "source": [
    "<font size=3><br> The truth is that not that much changed after the loudness feature has been added to the regression variables. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b809f",
   "metadata": {},
   "source": [
    "<font size=3><br> Now let's check what happens if we make a regression involving all of the song's features. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe2eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = list(feats.columns[2:13])\n",
    "all_columns.remove('valence')\n",
    "all_columns_formula = \"valence ~ \" + '+'.join(all_columns)\n",
    "all_columns_formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563a929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_ml_all = smf.ols(formula=all_columns_formula, data=feats)\n",
    "feats_res_ml_all = mod_ml_all.fit()\n",
    "feats_res_ml_all.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab40b6a",
   "metadata": {},
   "source": [
    "<font size=3><br> Now, let's try the interaction terms and squares, which could possibly help. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4573360",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_it = smf.ols(\"valence ~ danceability + np.power(danceability, 2)\", data=feats)\n",
    "feats_res_it = mod_it.fit()\n",
    "feats_res_it.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b751632b",
   "metadata": {},
   "source": [
    "<font size=3><br> Looks like the squares dont fit the problem that well.\n",
    "<br> Now running an ANOVA test. The null hypothesis is that the valence~energy model fits the problem better than the valence~{all of the features} one.\n",
    "<br> More on the ANOVA test here: https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/anova/ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b387d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = sm.stats.anova_lm(feats_res, feats_res_it)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0117d781",
   "metadata": {},
   "source": [
    "<font size=3><br> As we can see the p-value is equal to 1, which proves our null hypothesis. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b20b10f",
   "metadata": {},
   "source": [
    "<font size=3><br>Now let's perform a regression using interaction terms between the three most valence-correlated variables (danceability, energy and loudness). </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748575ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_inter = smf.ols(\"valence ~ danceability*energy*loudness\", data=feats)\n",
    "feats_res_inter = mod_inter.fit()\n",
    "feats_res_inter.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad502b",
   "metadata": {},
   "source": [
    "<font size=3><br> And finally an ANOVA test, where the null hypothesis is that the valence~energy model fits the problem better than the valence ~ danceability * energy * loudness problem. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de8d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = sm.stats.anova_lm(feats_res, feats_res_inter)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa73a0",
   "metadata": {},
   "source": [
    "<font size=3><br> The p-value is essentially equal to 0, which means that our null hypothesis is not true. So, the interaction terms model fits the problem better. This means that the three variables (danceability, energy, loudness) seem to be the most important ones. They play a bigger role on what the valence value will be. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa491ecc",
   "metadata": {},
   "source": [
    "<font size=3><br> Subsets now. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24a9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subset(y, data, feature_set):\n",
    "    X = data.loc[:, feature_set].values\n",
    "    X = sm.add_constant(X)\n",
    "    names = ['intercept']\n",
    "    names.extend(feature_set)\n",
    "    model = sm.OLS(y, X)\n",
    "    model.data.xnames = names\n",
    "    #print(model.data.xnames)\n",
    "    regr = model.fit()\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c2e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_best_of_k(y, data, k):\n",
    "    \n",
    "    best_rsquared = 0\n",
    "    best_model = None\n",
    "    for comb in itertools.combinations(data.columns[2:], k):\n",
    "        regr = process_subset(y, data, comb)\n",
    "        if regr.rsquared > best_rsquared:\n",
    "            best_rsquared = regr.rsquared\n",
    "            best_model = regr\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4931378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_subset_selection(data, exog):\n",
    "    best_model = None\n",
    "    best_models = []\n",
    "    y = data.loc[:, exog]\n",
    "    endog = [ x for x in data.columns[2:] if x != exog ]\n",
    "    X = data.loc[:, endog]\n",
    "\n",
    "    for i in range(1, len(data.columns[2:12])):\n",
    "        print(f'Finding the best model for {i} variable{\"s\" if i > 1 else \"\"}')\n",
    "        model = get_best_of_k(y, X, i)\n",
    "        if not best_model or model.rsquared_adj > best_model.rsquared_adj:\n",
    "            best_model = model\n",
    "        print(f'past that {i}')\n",
    "        print(model.model.data.xnames[1:]) # get the variables minums the intercept\n",
    "        best_models.append(model)\n",
    "\n",
    "    print(f'Fitted {2**len(data.columns)} models')\n",
    "    return best_model, best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c623ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, _ = best_subset_selection(feats, 'valence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b10b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_subsets = smf.ols(\"valence ~ loudness + duration_ms + speechiness + acousticness\", data=feats)\n",
    "feats_res_subsets = mod_subsets.fit()\n",
    "feats_res_subsets.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f6bd01",
   "metadata": {},
   "source": [
    "<font size=3><br>Overall, I come to the conclusion that the valence variable mostly depends on the danceability, energy and loudness features, which is more or less really reasonable. Every happy song that passes through my mind can be identified as a really danceable, energetic and -most of the times- loud song. Although, I think that valence could also be influenced from the song's lyrics, for example analyzing the lyrics using NLP algorithms and adding the output to the overall evaluation. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761f92e4",
   "metadata": {},
   "source": [
    "<font size=3><br> Q2 - Predict Valence</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab22924",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_ = feats.pop('tempo')\n",
    "feats.insert(11, 'tempo', move_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c9a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485d77d",
   "metadata": {},
   "source": [
    "<font size=3><br> KNN - The first non neural network algorithm that we are going to use is the K-Nearest Neighbours algorithm.\n",
    "<br> We split the dataset into train and test data using the train_test_split sklearn's function. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da1f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = feats.loc[:, 'danceability':'tempo'].values\n",
    "y = feats['valence'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbc5570",
   "metadata": {},
   "source": [
    "<font size=3><br>It is time to define the KNeighboursRegressor and then fit the model. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4444d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor(n_neighbors=6)\n",
    "neigh.fit(feats.loc[:, 'danceability':'tempo'].values, feats['valence'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5731f6",
   "metadata": {},
   "source": [
    "<font size=3><br> Let's see a prediction of the model. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8992bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neigh.predict([[0.497, 0.694, 8, -5.677, 1, 0.0478, 0.0811, 0.00001, 0.176, 202]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f244f",
   "metadata": {},
   "source": [
    "<font size=3><br> With the score function, we receive an evaluation of the model. More specifically, we receive the R-squared metric on the given samples. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh.score(X, y) #R squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b168f",
   "metadata": {},
   "source": [
    "<font size=3><br> Now, it is time to find which is the best value for K. We use the script below and evaluate each K based on the Root Mean Squared Error (RMSE) metric.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be614460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from numpy import sqrt\n",
    "\n",
    "rmse_val = [] #to store rmse values for different k\n",
    "for K in range(20):\n",
    "    K = K+1\n",
    "    model = KNeighborsRegressor(n_neighbors = K)\n",
    "\n",
    "    model.fit(X_train, y_train)  #fit the model\n",
    "    pred=model.predict(X_test) #make prediction on test set\n",
    "    error = sqrt(mean_squared_error(y_test,pred)) #calculate rmse\n",
    "    rmse_val.append(error) #store rmse values\n",
    "    print('RMSE value for k = ' , K , 'is:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8394198",
   "metadata": {},
   "source": [
    "<font size=3><br> The RMSE is averagely around 0.2. So best K is probaly K=6 or K=7.\n",
    "<br> Below there is a plot of the RMSE for the different values of K.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the rmse values against k values\n",
    "curve = pd.DataFrame(rmse_val) #elbow curve \n",
    "curve.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cbf1c0",
   "metadata": {},
   "source": [
    "<font size=3><br> So let's use the train and test data into the model. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d9284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsRegressor(n_neighbors = 7)\n",
    "\n",
    "model.fit(X_train, y_train)  #fit the model\n",
    "pred=model.predict(X_test) #make prediction on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb831c06",
   "metadata": {},
   "source": [
    "<font size=3><br> A new dataframe is created in order to store the knn estimation and the true valence for each song and see them side-by-side. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b457a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_estimator_2_df = pd.DataFrame(columns=['knn_estimation', 'true_valence'])\n",
    "valence_estimator_2_df['true_valence'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc179da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_estimator_2_df['knn_estimation'] = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d169a",
   "metadata": {},
   "source": [
    "<font size=3><br> So this is the ouput, and the look of the newly created dataframe. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985021c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_estimator_2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e88649",
   "metadata": {},
   "source": [
    "<font size=3>-----------------------------------------------------------------------------------------</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5093c237",
   "metadata": {},
   "source": [
    "<font size=3><br>Linear Regression - The second non neural network model that we are going to use is the linear regression.\n",
    "<br> For this purpose, we are once again using the sklearn library. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae6277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3388b65e",
   "metadata": {},
   "source": [
    "<font size=3><br> Once again, let's see the evaluation of the model. Firstly, on the train data. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3928fa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646eb80c",
   "metadata": {},
   "source": [
    "<font size=3><br> The R-squared value for the train data is 0.24 which is not really that good.\n",
    "<br> Let's now check the coefficients of the regression. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720b8609",
   "metadata": {},
   "source": [
    "<font size=3><br> And of course, the final evaluation, on the test data. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df6aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8797f3",
   "metadata": {},
   "source": [
    "<font size=3>-----------------------------------------------------------------------------------------</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c300a1",
   "metadata": {},
   "source": [
    "<font size=3><br>Decision Tree -  The third and final non neural network method that is going to be used on this assignment is the decision tree. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f08c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8cf4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness','tempo']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28a8552",
   "metadata": {},
   "source": [
    "<font size=3><br> For one more time, we preprocess the data and then splitting them apropriately into train and test sets. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64bc245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = feats.loc[:, cols].values\n",
    "y = feats['valence'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee2934",
   "metadata": {},
   "source": [
    "<font size=3><br> Below is the intialization of the decision tree. It is stated with a max_depth of 8, min_samples_leaf equal to 0.13 and random state = 3. We fit the train data into the decision tree. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ec074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtree = DecisionTreeRegressor(max_depth=8, min_samples_leaf=0.13, random_state=3)\n",
    "\n",
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca2c12c",
   "metadata": {},
   "source": [
    "<font size=3><br> Now let's use the decision tree in order to predict the valence of the songs. We are first using the model on the train data. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a1b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "pred_train_tree= dtree.predict(X_train)\n",
    "print(np.sqrt(mean_squared_error(y_train,pred_train_tree)))\n",
    "print(r2_score(y_train, pred_train_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10418600",
   "metadata": {},
   "source": [
    "<font size=3><br> As we can see, the RMSE is equal to 0.19 and the R-squared metric equal to 0.20. \n",
    "<br> Now let's use the tree on the test data. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b5881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_tree= dtree.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test,pred_test_tree))) \n",
    "print(r2_score(y_test, pred_test_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6072f664",
   "metadata": {},
   "source": [
    "<font size=3><br>As expected the R-squared value is lower than it was on the train data. \n",
    "<br> But there are some ways in order to improve the performance of the decision tree. We are now going to create a random forest.\n",
    "<br> First we import the RandomForestRegressor and then create the model. Of course we fit the train data into the model.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a87869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#RF model\n",
    "model_rf = RandomForestRegressor(n_estimators=500, oob_score=True, random_state=100)\n",
    "model_rf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59937814",
   "metadata": {},
   "source": [
    "<font size=3><br> Now let's check the efficiency of the Random Forest. First in the train data and then in the test data. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e137be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_rf= model_rf.predict(X_train)\n",
    "print(np.sqrt(mean_squared_error(y_train,pred_train_rf)))\n",
    "print(r2_score(y_train, pred_train_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893af062",
   "metadata": {},
   "source": [
    "<font size=3><br> Actually, the Random Forest Regressor is performing really well in the train data. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d0b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_rf = model_rf.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test,pred_test_rf)))\n",
    "print(r2_score(y_test, pred_test_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82c7c2",
   "metadata": {},
   "source": [
    "<font size=3><br> Moreover, the Random Forest Regressor is performing way better than the simple decision tree on the test data. There is a lower RMSE (=0.16) and much higher R-squared (=0.40). </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc7f45",
   "metadata": {},
   "source": [
    "<font size=3>Neural Network - It is now time to build a neural network model in order to estimate the value of the valence for all these songs.\n",
    "<br> The Tensorflow and Keras libraries are going to be used. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04020aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9437b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eebd1b6",
   "metadata": {},
   "source": [
    "<font size=3><br> Before building the model, we have to preprocess the data. As we can clearly see, there is a big variation between the different features' values. So we are going to normalize the data using the Normalization function that the Keras library is offering. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bffe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "normalizer = preprocessing.Normalization()\n",
    "normalizer.adapt(np.array(X_train_df))\n",
    "with np.printoptions(precision=2):\n",
    "    print(normalizer.mean)\n",
    "    print(normalizer.variance)\n",
    "    print(normalizer.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc2017",
   "metadata": {},
   "source": [
    "<font size=3><br>Now, it is time to build the model. After the normalization of the data, there will be 5 dense layers. The first layer will have 512 neurons and RELU as activation function. Layers 2 and 3 are almost the same but featuring 256 and 128 neurons correspondingly. The fourth layer is using 64 neurons and TANH as the activation function. Finally, the fifth layer is the output layer. \n",
    "<br> As the loss function, we will be using the Mean Absolute Error (MAE). As the optimizer, we are using Adam initialized with a value of 0.001. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c41c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_compile_model():\n",
    "    model = keras.Sequential([\n",
    "        normalizer,\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='tanh'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_compile_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72458d2f",
   "metadata": {},
   "source": [
    "<font size=3><br> Below is a summary of the Neural Network model. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c939aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f567e69",
   "metadata": {},
   "source": [
    "<font size=3><br> We are now going to fit the model for 100 epochs and using a 20% validation split. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_df, \n",
    "    y_train,\n",
    "    epochs=num_epochs, \n",
    "    validation_split=0.2,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5cce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f28c6",
   "metadata": {},
   "source": [
    "<font size=3><br> Here is a plot of the history. The evolution of the loss and val_loss during the epochs can be seen. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e36e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22904e5f",
   "metadata": {},
   "source": [
    "<font size=3><br> Now let's build the model again, but this time early stopping is going to be used, in order to prevent the model from overfittinng. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799dd1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_compile_model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(X_train_df, y_train, epochs=num_epochs,\n",
    "                    validation_split = 0.2, verbose=1, \n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b37f1",
   "metadata": {},
   "source": [
    "<font size=3><br> Finally, let's use the model on the test data and check its predictions. We are going to plot the predictions in the same diagram with the true valence in order to see the model's performance. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test).flatten()\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(y_test, test_predictions)\n",
    "plt.xlabel('True Valence')\n",
    "plt.ylabel('Prediction')\n",
    "lims = [0, 1]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5490c7",
   "metadata": {},
   "source": [
    "<font size=3><br> So overall, the model's performance is good. There is an RMSE of around 0.15 but as it can be seen on the plot above the predicted values are close to the y=x line  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007b8f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3dce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c72423b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cabf88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
